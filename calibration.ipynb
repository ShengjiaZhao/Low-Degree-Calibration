{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liberal-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from matplotlib import pyplot as plt\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "run_id = 10\n",
    "cold_start = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inside-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('dataset_adult.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expanded-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLinear(nn.Module):\n",
    "    def __init__(self, x_dim, out_dim=1):\n",
    "        super(NetworkLinear, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return torch.sigmoid(out) * 2 - 1  # Output range [-1, 1]\n",
    "    \n",
    "class NetworkFC2(nn.Module):\n",
    "    def __init__(self, x_dim, out_dim=1):\n",
    "        super(NetworkFC2, self).__init__()\n",
    "        h_dim = 50\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc2(F.leaky_relu(self.fc1(x)))\n",
    "        return torch.sigmoid(out) * 2 - 1  # Output range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bizarre-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLinearMC(nn.Module):\n",
    "    def __init__(self, x_dim, out_dim=1, n_bins=15):\n",
    "        super(NetworkLinearMC, self).__init__()\n",
    "        self.n_bins = n_bins\n",
    "        self.fc1 = nn.ModuleList([nn.Linear(x_dim, out_dim) for n in range(n_bins)])\n",
    "        \n",
    "    def forward(self, x, p):\n",
    "        with torch.no_grad():\n",
    "            binning = (p * self.n_bins).floor().clamp(max=self.n_bins-1).type(torch.int64)\n",
    "            binning = F.one_hot(binning, num_classes=self.n_bins)  # [batch_size, n_bins]\n",
    "        \n",
    "        out = torch.stack([fc(x) for fc in self.fc1], dim=1)   # [batch_size, n_bins, 1]\n",
    "        out = (binning.view(-1, self.n_bins, 1) * out).sum(dim=1)\n",
    "        return torch.sigmoid(out) * 2 - 1  # Output range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smaller-monte",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sailhome/zhaosj12/.pyenv/versions/3.8.3/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 105]) torch.Size([2000]) torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# Data spliting\n",
    "torch.manual_seed(run_id)\n",
    "\n",
    "train_size = 5000   # When run_id = 0-9 use 1000, otherwise us 5000\n",
    "val_size = 1000\n",
    "test_size = 2000\n",
    "feat_train, prob_train = dataset.generate(train_size)\n",
    "feat_train, prob_train = feat_train.to(device), prob_train.to(device).flatten()\n",
    "label_train = (torch.rand_like(prob_train) < prob_train).type(torch.float32)\n",
    "\n",
    "feat_val, prob_val = dataset.generate(val_size)\n",
    "feat_val, prob_val = feat_val.to(device), prob_val.to(device).flatten()\n",
    "label_val = (torch.rand_like(prob_val) < prob_val).type(torch.float32)\n",
    "\n",
    "feat_test, prob_test = dataset.generate(test_size)\n",
    "feat_test, prob_test = feat_test.to(device), prob_test.to(device).flatten()\n",
    "label_test = (torch.rand_like(prob_test) < prob_test).type(torch.float32)\n",
    "\n",
    "feat = torch.cat([feat_train, feat_val, feat_test], dim=0)\n",
    "\n",
    "print(feat_test.shape, prob_test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faced-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6545, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.5044, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.4122, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.4120, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3178, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2896, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1757, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1429, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.1071, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.0618, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize from overfitted\n",
    "\n",
    "from synthetic import *\n",
    "\n",
    "train_dataset = TensorDataset(feat_val.cpu(), label_val.cpu())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net_init = NetworkFC(x_dim=feat_val.shape[1]).to(device)\n",
    "optim = torch.optim.Adam(net_init.parameters(), lr=1e-3) \n",
    "for epoch in range(100):\n",
    "    for bx, by in train_loader:\n",
    "        bx = bx.to(device)\n",
    "        by = by.to(device).to(torch.float32)\n",
    "        optim.zero_grad()\n",
    "\n",
    "        pred = net_init(bx)\n",
    "        # loss = pred.pow(2).mean() * 0.02\n",
    "        loss = F.binary_cross_entropy(input=F.sigmoid(pred).flatten(), target=by)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 10 == 0:    \n",
    "        print(loss)\n",
    "    \n",
    "\n",
    "    # print(((outcome0 - cur_pred) * (category * 2 - 1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eleven-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_var(feature, prediction, prob, critic_network=NetworkLinear):\n",
    "    net = critic_network(x_dim=feature.shape[1]).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', patience=3, threshold=1e-4, factor=0.5)\n",
    "    \n",
    "    bin_labels = (torch.rand_like(prob) < prob).type(torch.float32)\n",
    "    for iteration in range(10000):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        weight = (net(feature).flatten() + 1) / 2   # Output range is [0, 1]\n",
    " \n",
    "        reg = (1e-5 + weight).log().mean() + (1 - weight + 1e-5).log().mean()  # Regularize the weights away from 0 or 1 for better stability\n",
    "        prob_mean = (weight * prob).mean() / (1e-5 + weight.mean())       # Mean true probability of the selected group \n",
    "        pred_mean = (weight * prediction).mean() / (1e-5 + weight.mean()) # Mean prediction of the selected group \n",
    "        prob_var = weight * (prob - prob_mean) ** 2   # Variance of the prob selected group \n",
    "        pred_var = weight * (prediction - pred_mean) ** 2  # Variance of the prediction for the selected group \n",
    "        loss = pred_var.mean() - prob_var.mean()   # Loss is variance on the true prob - variance on the prediction \n",
    "        \n",
    "        (-loss-0.001 * reg).backward()\n",
    "        optim.step()\n",
    "        lr_scheduler.step(-loss)\n",
    "        \n",
    "        # Hitchhike the lr scheduler to terminate if no progress\n",
    "        if optim.param_groups[0]['lr'] < 1e-6:   \n",
    "            break\n",
    "            \n",
    "#         if iteration % 100 == 0:\n",
    "#             print(loss, weight[:10])\n",
    "#             plt.hist(weight.detach().cpu().numpy())\n",
    "#             plt.show()\n",
    "    return loss\n",
    "\n",
    "#         if iteration % 100 == 0:\n",
    "#             print(prob_var.mean(), pred_var.mean())\n",
    "    \n",
    "# feat_test, prob_test = dataset.generate(500)\n",
    "# prob_test = torch.ones_like(prob_test) * 0.5\n",
    "# bin_test = (torch.rand_like(prob_test) < prob_test).type(torch.float32)\n",
    "# evaluate_var(feat_test, pred_test, prob_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "informal-republican",
   "metadata": {},
   "source": [
    "plt.hist(prob_test.cpu().numpy(), bins=20)\n",
    "print(((pred_test - pred_test.mean()) ** 2).mean())\n",
    "print(((prob_test - prob_test.mean()) ** 2).mean())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "neutral-costa",
   "metadata": {},
   "source": [
    "print(pred_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollywood-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiaccuracy(feature, prediction, label, critic_network=NetworkLinear):\n",
    "    \"\"\" Compute the multi-accuracy error under a some critic network\n",
    "    \n",
    "    Args:\n",
    "        feature (array [batch_size, n_feature]): the covariates\n",
    "        prediction (array [batch_size]): the predicted probability in [0, 1]\n",
    "        label (array [batch_size]): the binary labels\n",
    "        critic_network (torch.Module class)\n",
    "    \"\"\"\n",
    "    net = critic_network(x_dim=feature.shape[1]).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', patience=3, threshold=1e-5, factor=0.5)\n",
    "    \n",
    "    for iteration in range(1000):\n",
    "        optim.zero_grad()\n",
    "        pred = net(feature).flatten() \n",
    "        loss = pred * (label - prediction) \n",
    "        loss = loss.mean()\n",
    "        (-loss).backward()\n",
    "        optim.step()\n",
    "        lr_scheduler.step(-loss)\n",
    "        \n",
    "        # Hitchhike the lr scheduler to terminate if no progress\n",
    "        if optim.param_groups[0]['lr'] < 1e-6:   \n",
    "            break\n",
    "                \n",
    "    pred_bin = (pred > 0).type(torch.float32) * 2 - 1\n",
    "    loss_actual = pred_bin * (label - prediction)\n",
    "    return loss_actual.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gross-courage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1104/0.0000/0.1184, 0.0000/0.0000/0.0967\n",
      "tensor(0.1406, device='cuda:0', grad_fn=<DivBackward0>) tensor(1265.2932, device='cuda:0')\n",
      "0.0860/0.0000/0.1228, 0.0000/0.0000/0.0554\n",
      "tensor(0.1022, device='cuda:0', grad_fn=<DivBackward0>) tensor(1113.9291, device='cuda:0')\n",
      "0.0676/0.0000/0.0883, 0.0000/0.0000/0.0615\n",
      "tensor(0.0838, device='cuda:0', grad_fn=<DivBackward0>) tensor(1029.9114, device='cuda:0')\n",
      "0.0612/0.0000/0.0974, 0.0000/0.0000/0.0508\n",
      "tensor(0.0761, device='cuda:0', grad_fn=<DivBackward0>) tensor(982.9684, device='cuda:0')\n",
      "0.0572/0.0000/0.0984, 0.0000/0.0000/0.0479\n",
      "tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>) tensor(947.4574, device='cuda:0')\n",
      "0.0533/0.0000/0.0930, 0.0000/0.0000/0.0521\n",
      "tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>) tensor(919.0826, device='cuda:0')\n",
      "0.0455/0.0000/0.0994, 0.0000/0.0000/0.0401\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>) tensor(898.0526, device='cuda:0')\n",
      "0.0463/0.0000/0.0895, 0.0000/0.0000/0.0439\n",
      "tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>) tensor(880.3240, device='cuda:0')\n",
      "0.0424/0.0000/0.0954, 0.0000/0.0000/0.0397\n",
      "tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>) tensor(863.2616, device='cuda:0')\n",
      "0.0425/0.0000/0.0892, 0.0000/0.0000/0.0431\n",
      "tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>) tensor(848.9852, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "# Initialize the predictions\n",
    "if cold_start:\n",
    "    pred_train = torch.ones(train_size, device=device) * 0.5\n",
    "    pred_test = torch.ones(test_size, device=device) * 0.5\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        pred_train = F.sigmoid(net_init(feat_train)).flatten()\n",
    "        pred_test = F.sigmoid(net_init(feat_test)).flatten()\n",
    "    \n",
    "var_train, var_test, var_val = [0], [0], [0]\n",
    "ma_train, ma_test, ma_val = [0], [0] ,[0]\n",
    "\n",
    "for rep in range(10):\n",
    "    # Keep the current largest validation set error \n",
    "    loss_best_val = -100\n",
    "    patience = 0\n",
    "\n",
    "    # Evaluate the multi-accuracy\n",
    "    ma_train.append(evaluate_multiaccuracy(feature=feat_train, prediction=pred_train, label=label_train))\n",
    "    ma_test.append(evaluate_multiaccuracy(feature=feat_test, prediction=pred_test, label=label_test))\n",
    "    \n",
    "    # Evaluate the maximum variance gap \n",
    "    # var_train.append(evaluate_var(feature=feat_train, prediction=pred_train, prob=prob_train))\n",
    "    # var_val.append(evaluate_var(feature=feat_val, prediction=pred_val, prob=prob_val))\n",
    "    var_test.append(evaluate_var(feature=feat_test, prediction=pred_test, prob=prob_test))\n",
    "    # test_var.append(0)\n",
    "    print(\"%.4f/%.4f/%.4f, %.4f/%.4f/%.4f\" % \n",
    "          (ma_train[-1], ma_val[-1], ma_test[-1], var_train[-1], var_val[-1], var_test[-1]))\n",
    "    \n",
    "    net = NetworkLinear(x_dim=feat_train.shape[1]).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "    \n",
    "    # Do SGD for better convergence properties\n",
    "    train_dataset = TensorDataset(feat_train.cpu(), torch.stack([label_train, pred_train], dim=1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        for bx, by in train_loader:\n",
    "            bx = bx.to(device)\n",
    "            bl = by[:, 0].to(device)\n",
    "            bp = by[:, 1].to(device)\n",
    "            optim.zero_grad()\n",
    "\n",
    "            weight = net(bx).flatten()   # array [-1, 1]\n",
    "            loss = (weight * (bl - bp)).mean()   # weight should be +1 when label > pred and -1 when label < pred\n",
    "            (-loss).backward()    # Maximize the loss \n",
    "            optim.step()\n",
    "        \n",
    "        # Use the validation set to determine early stopping\n",
    "        with torch.no_grad():\n",
    "            # Evaluate the validation multi-accuracy error\n",
    "            weight_train = net(feat_train).flatten()\n",
    "            loss_val = (weight_train * (label_train - pred_train)).mean()\n",
    "\n",
    "            # Select the maximum validation error \n",
    "            if loss_val > loss_best_val + 1e-5:\n",
    "                patience = 0\n",
    "                best_net = copy.deepcopy(net)\n",
    "                loss_best_val = loss_val\n",
    "            else:   # If no improvement for 5 iter then exit\n",
    "                patience += 1\n",
    "                if patience > 5:\n",
    "                    break\n",
    "    \n",
    "    # Choose optimal lr\n",
    "    weight_train = best_net(feat_train).flatten()\n",
    "    lr = (weight_train * (label_train - pred_train)).sum() / (weight_train ** 2).sum()\n",
    "    print(lr, ((pred_train - label_train) ** 2).sum())\n",
    "    # print(net(feat_val)[:10])\n",
    "    # lr = 1e-1 * 0.8 ** rep\n",
    "    with torch.no_grad():\n",
    "        pred_train = pred_train + lr * best_net(feat_train).flatten() \n",
    "        pred_test = pred_test + lr * best_net(feat_test).flatten()\n",
    "    \n",
    "        pred_train = pred_train.clamp(min=0, max=1)\n",
    "        pred_test = pred_test.clamp(min=0, max=1)\n",
    "        \n",
    "    # print(((pred_train - label_train) ** 2).sum())\n",
    "    # print(pred_test[:10])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abroad-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'ma_train': ma_train, 'ma_test': ma_test,\n",
    "        'var_train': var_train, 'var_test': var_test}\n",
    "import pickle\n",
    "with open('results/var/ma-noval-%r-run=%d.pickle' % (cold_start, run_id), 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-slovenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1109/0.0000/0.1185, 0.0000/0.0000/0.0967\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "# Initialize the predictions\n",
    "if cold_start:\n",
    "    pred_train = torch.ones(train_size, device=device) * 0.5\n",
    "    pred_test = torch.ones(test_size, device=device) * 0.5\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        pred_train = F.sigmoid(net_init(feat_train)).flatten()\n",
    "        pred_test = F.sigmoid(net_init(feat_test)).flatten()\n",
    "    \n",
    "var_train, var_test, var_val = [0], [0], [0]\n",
    "ma_train, ma_test, ma_val = [0], [0] ,[0]\n",
    "\n",
    "for rep in range(10):\n",
    "    # Keep the current largest validation set error \n",
    "    loss_best_val = -100\n",
    "    patience = 0\n",
    "\n",
    "    # Evaluate the multi-accuracy\n",
    "    ma_train.append(evaluate_multiaccuracy(feature=feat_train, prediction=pred_train, label=label_train))\n",
    "    ma_test.append(evaluate_multiaccuracy(feature=feat_test, prediction=pred_test, label=label_test))\n",
    "    \n",
    "    # Evaluate the maximum variance gap \n",
    "#     var_train.append(evaluate_var(feature=feat_train, prediction=pred_train, prob=prob_train))\n",
    "#     var_val.append(evaluate_var(feature=feat_val, prediction=pred_val, prob=prob_val))\n",
    "    var_test.append(evaluate_var(feature=feat_test, prediction=pred_test, prob=prob_test))\n",
    "    # test_var.append(0)\n",
    "    print(\"%.4f/%.4f/%.4f, %.4f/%.4f/%.4f\" % \n",
    "          (ma_train[-1], ma_val[-1], ma_test[-1], var_train[-1], var_val[-1], var_test[-1]))\n",
    "    \n",
    "    net1 = NetworkLinear(x_dim=feat_train.shape[1]).to(device)\n",
    "    net2 = NetworkLinear(x_dim=feat_train.shape[1]).to(device)\n",
    "    net = [net1, net2]\n",
    "    optim = [torch.optim.Adam(net_.parameters(), lr=1e-3) for net_ in net]\n",
    "    \n",
    "    # Do SGD for better convergence properties\n",
    "    train_dataset = TensorDataset(feat_train.cpu(), torch.stack([label_train, pred_train], dim=1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    if rep % 3 == 0:\n",
    "        choice = 0\n",
    "    else:\n",
    "        choice = 1\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        for bx, by in train_loader:\n",
    "            bx = bx.to(device)\n",
    "            bl = by[:, 0].to(device)\n",
    "            bp = by[:, 1].to(device)\n",
    "            optim[choice].zero_grad()\n",
    "\n",
    "            weight = net[choice](bx).flatten()   # array [-1, 1]\n",
    "            if choice == 0:\n",
    "                loss = (weight * (bl - bp)).mean()\n",
    "            else:\n",
    "                loss = (weight * bp * (bl - bp)).mean()\n",
    "            # loss = torch.maximum(loss1, loss2)   # weight should be +1 when label > pred and -1 when label < pred\n",
    "            (-loss).backward()    # Maximize the loss \n",
    "            optim[choice].step()\n",
    "        \n",
    "        # Use the validation set to determine early stopping\n",
    "        with torch.no_grad():\n",
    "            # Evaluate the validation multi-accuracy error\n",
    "            weight_val = net[choice](feat_train).flatten()\n",
    "            if choice == 0:\n",
    "                loss_val = (weight_val * (label_train - pred_train)).mean()\n",
    "            else:\n",
    "                loss_val = (weight_val * pred_train * (label_train - pred_train)).mean()\n",
    "            # loss_val = torch.maximum(loss_val1, loss_val2)\n",
    "                \n",
    "            # Select the maximum validation error \n",
    "            if loss_val > loss_best_val + 1e-5:\n",
    "                patience = 0\n",
    "                best_net = copy.deepcopy(net[choice])\n",
    "                loss_best_val = loss_val\n",
    "                # choice = loss_val1 > loss_val2\n",
    "            else:   # If no improvement for 5 iter then exit\n",
    "                patience += 1\n",
    "                if patience > 5:\n",
    "                    break\n",
    "\n",
    "    \n",
    "    # Choose optimal lr\n",
    "    weight_val = best_net(feat_train).flatten()\n",
    "    if choice == 0:\n",
    "        lr = (weight_val * (label_train - pred_train)).sum() / (weight_val ** 2).sum()\n",
    "    else:\n",
    "        lr = (weight_val * pred_train * (label_train - pred_train)).sum() / ((weight_val * pred_train) ** 2).sum()\n",
    "    print(choice, lr, ((pred_train - label_train) ** 2).sum(), loss_best_val)\n",
    "    # print(net(feat_val)[:10])\n",
    "    # lr = 1e-1 * 0.8 ** rep\n",
    "    with torch.no_grad():\n",
    "        if choice == 0:\n",
    "            pred_train = pred_train + lr * best_net(feat_train).flatten() \n",
    "            pred_test = pred_test + lr * best_net(feat_test).flatten()\n",
    "        else:\n",
    "            pred_train = pred_train + lr * best_net(feat_train).flatten() * pred_train\n",
    "            pred_test = pred_test + lr * best_net(feat_test).flatten() * pred_test\n",
    "    \n",
    "        pred_train = pred_train.clamp(min=0, max=1)\n",
    "        pred_test = pred_test.clamp(min=0, max=1)\n",
    "        \n",
    "    # print(((pred_train - label_val) ** 2).sum())\n",
    "    # print(pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'ma_train': ma_train, 'ma_test': ma_test,\n",
    "        'var_train': var_train, 'var_test': var_test}\n",
    "import pickle\n",
    "with open('results/var/alternate-noval-%r-run=%d.pickle' % (cold_start, run_id), 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "\n",
    "# Initialize the predictions\n",
    "if cold_start:\n",
    "    pred_train = torch.ones(train_size, device=device) * 0.5\n",
    "    pred_test = torch.ones(test_size, device=device) * 0.5\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        pred_train = F.sigmoid(net_init(feat_train)).flatten()\n",
    "        pred_test = F.sigmoid(net_init(feat_test)).flatten()\n",
    "\n",
    "var_train, var_test, var_val = [0], [0], [0]\n",
    "ma_train, ma_test, ma_val = [0], [0] ,[0]\n",
    "\n",
    "for rep in range(10):\n",
    "    # Keep the current largest validation set error \n",
    "    loss_best_val = -100\n",
    "    patience = 0\n",
    "\n",
    "    # Evaluate the multi-accuracy\n",
    "    ma_train.append(evaluate_multiaccuracy(feature=feat_train, prediction=pred_train, label=label_train))\n",
    "    ma_test.append(evaluate_multiaccuracy(feature=feat_test, prediction=pred_test, label=label_test))\n",
    "    \n",
    "    # Evaluate the maximum variance gap \n",
    "    # var_train.append(evaluate_var(feature=feat_train, prediction=pred_train, prob=prob_train))\n",
    "    # var_val.append(evaluate_var(feature=feat_val, prediction=pred_val, prob=prob_val))\n",
    "    var_test.append(evaluate_var(feature=feat_test, prediction=pred_test, prob=prob_test))\n",
    "    # test_var.append(0)\n",
    "    print(\"%.4f/%.4f/%.4f, %.4f/%.4f/%.4f\" % \n",
    "          (ma_train[-1], ma_val[-1], ma_test[-1], var_train[-1], var_val[-1], var_test[-1]))\n",
    "    \n",
    "    net = NetworkLinearMC(x_dim=feat_train.shape[1]).to(device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "    \n",
    "    # Do SGD for better convergence properties\n",
    "    train_dataset = TensorDataset(feat_train.cpu(), torch.stack([label_train, pred_train], dim=1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        for bx, by in train_loader:\n",
    "            bx = bx.to(device)\n",
    "            bl = by[:, 0].to(device)\n",
    "            bp = by[:, 1].to(device)\n",
    "            optim.zero_grad()\n",
    "\n",
    "            weight = net(bx, bp).flatten()   # array [-1, 1]\n",
    "            loss = (weight * (bl - bp)).mean()   # weight should be +1 when label > pred and -1 when label < pred\n",
    "            (-loss).backward()    # Maximize the loss \n",
    "            optim.step()\n",
    "        \n",
    "        # Use the validation set to determine early stopping\n",
    "        with torch.no_grad():\n",
    "            # Evaluate the validation multi-accuracy error\n",
    "            weight_train = net(feat_train, pred_train).flatten()\n",
    "            loss_val = (weight_train * (label_train - pred_train)).mean()\n",
    "\n",
    "            # Select the maximum validation error \n",
    "            if loss_val > loss_best_val + 1e-5:\n",
    "                patience = 0\n",
    "                best_net = copy.deepcopy(net)\n",
    "                loss_best_val = loss_val\n",
    "            else:   # If no improvement for 5 iter then exit\n",
    "                patience += 1\n",
    "                if patience > 5:\n",
    "                    break\n",
    "    \n",
    "    # Choose optimal lr\n",
    "    weight_train = best_net(feat_train, pred_train).flatten()\n",
    "    lr = (weight_train * (label_train - pred_train)).sum() / (weight_train ** 2).sum()\n",
    "    print(lr, ((pred_train - label_train) ** 2).sum())\n",
    "    # print(net(feat_val)[:10])\n",
    "    # lr = 1e-1 * 0.8 ** rep\n",
    "    with torch.no_grad():\n",
    "        pred_train = pred_train + lr * best_net(feat_train, pred_train).flatten() \n",
    "        pred_test = pred_test + lr * best_net(feat_test, pred_test).flatten()\n",
    "        \n",
    "        pred_train = pred_train.clamp(min=0, max=1)\n",
    "        pred_test = pred_test.clamp(min=0, max=1)\n",
    "#     print(((pred_train - label_train) ** 2).sum())\n",
    "    # print(pred_test[:10])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'ma_train': ma_train, 'ma_test': ma_test,\n",
    "        'var_train': var_train, 'var_test': var_test}\n",
    "import pickle\n",
    "with open('results/var/mc-noval-%r-run=%d.pickle' % (cold_start, run_id), 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abstract-thursday",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_ma, label='test')\n",
    "# plt.plot(test_var, label='test1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "operating-boating",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_ma, label='train')\n",
    "plt.plot(val_ma, label='val')\n",
    "plt.plot(test_ma, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "careful-vaccine",
   "metadata": {},
   "source": [
    "    # Compute the adjustment for the set where weight > 0 and the set where weight < 0\n",
    "    category = best_net(feat_val).flatten() > 0\n",
    "    adjustment_0 = ((label_val - pred_val) * category).sum() / (1 + category.sum())  # Set where weight > 0  \n",
    "    adjustment_1 = ((label_val - pred_val)[~category].sum() / (1 + category.sum()) # Set where weight < 0\n",
    "    \n",
    "    print(loss_best_val, adjustment_0, adjustment_1)\n",
    "    # Apply the adjustment to the train, val, test data respectively \n",
    "    adjustment = (best_net(feat_train).flatten() > 0).type(torch.float32) \n",
    "    pred_train = pred_train + category * adjustment_0 + (1 - category) * adjustment_1 \n",
    "    \n",
    "    category = (best_net(feat_val).flatten() > 0).type(torch.float32) \n",
    "    pred_val = pred_val + category * adjustment_0 + (1 - category) * adjustment_1 \n",
    "    \n",
    "    category = (best_net(feat_test).flatten() > 0).type(torch.float32) \n",
    "    pred_test = pred_test + category * adjustment_0 + (1 - category) * adjustment_1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
